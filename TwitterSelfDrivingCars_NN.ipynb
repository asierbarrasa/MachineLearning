{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P18 URLCategorization\n",
    "***\n",
    "### Authors: Asier Barrasa and Iván Valdés \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "The aim of this project is to clasiffy a set of tweets among 5 diferent classess according to the sentiment toward self-driving cars.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is done in the Notebook: \n",
    "\n",
    "In this notebook, first we preprocess the given data-set int a cleaned version of it. Then we implement two NN's to solve the problem and finnaly, we compute the accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asier\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import multiprocessing\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import pandas as pd\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn import utils\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer , CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# imports for doc2vec\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import LabeledSentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleanning tweets\n",
    "def clean_tweets(data_set):\n",
    "    for index in range(len(data_set['text'])):\n",
    "        if index % 100 ==0:\n",
    "            print(index)\n",
    "        if index==409:\n",
    "            print(\"ahora\")\n",
    "        if(pd.isna(data_set['text'][index]) ):\n",
    "            print(\"el tweet es \" + data_set['text'][index] + \" de indice \"+ str(index))\n",
    "            data_set.drop(data_set.index[index], inplace=True)\n",
    "        else:  \n",
    "            tweet= data_set['text'][index]\n",
    "            cleaned_tweet=clean_tweet(tweet)\n",
    "            #print(cleaned_tweet)\n",
    "            data_set['text'][index] = cleaned_tweet\n",
    "        data_set.drop(data_set[df2.text.isna()].index, inplace=True) #Remove empty cells\n",
    "    return data_set\n",
    "\n",
    "# funcion for clean each tweet\n",
    "def clean_tweet(tweet):\n",
    "    # Convert text to lower\n",
    "    tweet = tweet.lower()\n",
    "    #Remove urls\n",
    "    regex_url = re.compile('(http:\\/\\/www\\.|https:\\/\\/www\\.|http:\\/\\/|https:\\/\\/)?[a-z0-9]+([\\-\\.]{1}[a-z0-9]+)*\\.[a-z]{2,5}(:[0-9]{1,5})?(\\/.*)')\n",
    "    tweet= re.sub(regex_url,'',tweet)\n",
    "    # Removing non ASCII chars\n",
    "    tweet = re.sub(r'[^\\x00-\\x7f]', r' ', tweet)\n",
    "    # Remove the punctuation\n",
    "    tweet = gensim.parsing.preprocessing.strip_punctuation2(tweet)\n",
    "    # Strip all the numerics\n",
    "    tweet = gensim.parsing.preprocessing.strip_numeric(tweet)\n",
    "    # Strip multiple whitespaces\n",
    "    tweet = gensim.corpora.textcorpus.strip_multiple_whitespaces(tweet)\n",
    "    words= tweet.split()\n",
    "    # Remove the english stopwords.Before this call, it is necessary to call to the function nltk.download('stopwords')\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "    # Remove the words with less than 3 letters\n",
    "    words = gensim.corpora.textcorpus.remove_short(words, minsize=3)\n",
    "    return (' '.join(words))\n",
    "\n",
    "# function for printing stats\n",
    "def printStats(estimator,name,x_train, y_train, x_test, y_test):\n",
    "    print(82 * '_')\n",
    "    t0=time()\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "        t0 = time()\n",
    "        sentiment_fit = estimator.fit(x_train, y_train)\n",
    "        y_pred = sentiment_fit.predict(x_test)\n",
    "        train_test_time = time() - t0\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print('name\\t\\ttime\\taccuracy')\n",
    "        print('%-9s\\t%.2fs\\t%.2f\\t'\n",
    "              % (name, (time() - t0), (accuracy*100)))\n",
    "       \n",
    "    \n",
    "def clean_data_set():    \n",
    "    data_set = pd.read_csv('data-set.csv',encoding='latin1')\n",
    "    cleaned_data_set = clean_tweets(data_set)\n",
    "    df2 = cleaned_data_set[cleaned_data_set.sentiment!='not_relevant']\n",
    "    #print(df[['sentiment','text']].sample(5))\n",
    "    df2 = pd.DataFrame(df2[['text','sentiment']])\n",
    "    df2.dropna()\n",
    "    df2.to_csv('cleaned_data_set.csv',columns=['text','sentiment'])\n",
    "    \n",
    "def show_data_set_stats(df):\n",
    "    \n",
    "    print(\"Number of data by class\")\n",
    "    print(\"Class 1 -->\", len(df[df.sentiment==1]))\n",
    "    print(\"Class 2 -->\",len(df[df.sentiment==2]))\n",
    "    print(\"Class 3 -->\",len(df[df.sentiment==3]))\n",
    "    print(\"Class 4 -->\",len(df[df.sentiment==4]))\n",
    "    print(\"Class 5 -->\",len(df[df.sentiment==5]))\n",
    "    \n",
    "# Transform the data on a one-hot vector\n",
    "def oneHotVector(data, numClasses):\n",
    "    targets = np.array(data).reshape(-1)\n",
    "    data = np.eye(numClasses)[targets-1]\n",
    "    return data\n",
    "\n",
    "def labelize_tweets_ug(tweets,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    count = 0\n",
    "    for t in  (tweets): \n",
    "        result.append(LabeledSentence(t.split(), [prefix + '_%s' % count]))\n",
    "        count = count+1\n",
    "    return result\n",
    "\n",
    "def get_vectors(model, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = model.docvecs[prefix]\n",
    "        n = n+1\n",
    "    return vecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncomment the function below for clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data by class\n",
      "Class 1 --> 110\n",
      "Class 2 --> 685\n",
      "Class 3 --> 4227\n",
      "Class 4 --> 1441\n",
      "Class 5 --> 458\n"
     ]
    }
   ],
   "source": [
    "#clean_data_set() \n",
    "df2 = pd.read_csv('cleaned_data_set.csv',encoding='latin1')\n",
    "show_data_set_stats(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split on test and trainning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df3 = pd.read_csv('cleaned_data_set.csv', encoding=\"latin1\")\n",
    "x = df2.text\n",
    "y = df2.sentiment\n",
    "\n",
    "x_train = x[:5000]\n",
    "y_train = y[:5000]\n",
    "y_train_NN = oneHotVector(data=y_train,numClasses=5)\n",
    "\n",
    "\n",
    "x_validation = x[-1921:]\n",
    "y_validation = y[-1921:]\n",
    "y_validation_NN = oneHotVector(data=y_validation,numClasses=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfVectorizer = TfidfVectorizer()\n",
    "cntVectorizer = CountVectorizer()\n",
    "\n",
    "#Set parameters for bigram and trigram\n",
    "\n",
    "# tfidfVectorizer.set_params(ngram_range=(1,2))\n",
    "# cntVectorizer.set_params(ngram_range=(1,2))\n",
    "\n",
    "x_train_tfidVectorized = tfidfVectorizer.fit_transform(x_train).toarray()\n",
    "x_validation_tfidVectorized = tfidfVectorizer.transform(x_validation).toarray()\n",
    "\n",
    "x_train_cntVectorized = cntVectorizer.fit_transform(x_train).toarray()\n",
    "x_validation_cntVectorized = cntVectorizer.transform(x_validation).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get spplited version of each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the whole data set for training  Doc2Vec. You cand find the complete document [here](https://arxiv.org/pdf/1607.05368.pdf).\n",
    "\n",
    "> We use all documents in the development and test\n",
    "set (and potentially more background documents,\n",
    "where  explicitly  mentioned)  to  train\n",
    "doc2vec\n",
    ".\n",
    "Our rationale for this is that the\n",
    "doc2vec\n",
    "training\n",
    "is  completely  unsupervised,\n",
    ">\n",
    "> -- <cite>Jey Han Lau and Timothy Baldwin</cite>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x_w2v = labelize_tweets_ug(df2.text, 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing DBOW (Distributed Bag Of Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dbow = Doc2Vec(dm=0,size=400, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "# Build the vocabulary using labelized tweets\n",
    "model_ug_dbow.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dbow.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dbow.alpha -= 0.002\n",
    "    model_ug_dbow.min_alpha = model_ug_dbow.alpha\n",
    "    \n",
    "\n",
    "# get vectors from the labelized tweets\n",
    "train_vecs_dbow = get_vectors(model_ug_dbow, x_train, 400)\n",
    "validation_vecs_dbow = get_vectors(model_ug_dbow, x_validation, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing DMC (Distributed Memory Concatenation)\n",
    "\n",
    "Parameters taken from [here](http://proceedings.mlr.press/v32/le14.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6921/6921 [00:00<00:00, 1260805.16it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1155650.22it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1260695.65it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 924512.82it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 990506.64it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1386747.14it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1155604.22it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1066646.26it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1260586.16it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 990506.64it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 866789.43it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 990574.24it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 866711.79it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 990506.64it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 924483.38it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 924395.06it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 815758.83it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1066646.26it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 924512.82it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 815712.98it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 866685.91it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 770607.33it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 990506.64it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1155696.23it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 729879.76it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1066685.46it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 1541378.32it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 990574.24it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 924512.82it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 770484.61it/s]\n",
      "100%|██████████| 6921/6921 [00:00<00:00, 815781.76it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5653305570015617"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%capture\n",
    "\n",
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_dmc = Doc2Vec(dm=1, dm_concat=1, size=400, window=10, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_dmc.build_vocab([x for x in tqdm(all_x_w2v)])\n",
    "\n",
    "for epoch in range(30):\n",
    "    model_ug_dmc.train(utils.shuffle([x for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_dmc.alpha -= 0.002\n",
    "    model_ug_dmc.min_alpha = model_ug_dmc.alpha\n",
    "train_vecs_dmc = get_vectors(model_ug_dmc, x_train, 400)\n",
    "validation_vecs_dmc = get_vectors(model_ug_dmc, x_validation, 400)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dmc, y_train)\n",
    "clf.score(validation_vecs_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6007287870900573"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_concat_vectors(model1,model2, corpus, size):\n",
    "    vecs = np.zeros((len(corpus), size))\n",
    "    n = 0\n",
    "    for i in corpus.index:\n",
    "        prefix = 'all_' + str(i)\n",
    "        vecs[n] = np.append(model1.docvecs[prefix],model2.docvecs[prefix])\n",
    "        n += 1\n",
    "    return vecs\n",
    "\n",
    "train_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_train, 800)\n",
    "validation_vecs_dbow_dmc = get_concat_vectors(model_ug_dbow,model_ug_dmc, x_validation, 800)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train_vecs_dbow_dmc, y_train)\n",
    "clf.score(validation_vecs_dbow_dmc, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________\n",
      "name\t\ttime\taccuracy\n",
      "vecs_dbow\t4.34s\t60.07\t\n",
      "__________________________________________________________________________________\n",
      "name\t\ttime\taccuracy\n",
      "tfidVectorized\t0.26s\t59.86\t\n",
      "__________________________________________________________________________________\n",
      "name\t\ttime\taccuracy\n",
      "cntVectorized\t0.70s\t60.49\t\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "printStats(clf,'vecs_dbow',train_vecs_dbow_dmc, y_train,validation_vecs_dbow_dmc, y_validation)\n",
    "printStats(clf,'tfidVectorized',x_train_tfidVectorized, y_train,x_validation_tfidVectorized, y_validation)\n",
    "printStats(clf,'cntVectorized',x_train_cntVectorized, y_train,x_validation_cntVectorized, y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN for _doc2vec_\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 30\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 800  # 800 inputs\n",
    "n_classes = 5 # 5 classes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(\"float\", [None, n_input]) \n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "\n",
    "    print( 'x:', x.get_shape(), 'W1:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())        \n",
    "    # Hidden layer with linear activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    # Hidden layer with linear activation\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W2:', weights['h2'].get_shape(), 'b2:', biases['b2'].get_shape())        \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Output layer with Softmax activation\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W3:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']   \n",
    "    out_layer = tf.nn.softmax(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),    #800x256\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #256x256\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))  #256x5\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #256x1\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #256x1\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))              #5x1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (?, 800) W1: (800, 256) b1: (256,)\n",
      "layer_1: (?, 256) W2: (256, 256) b2: (256,)\n",
      "layer_2: (?, 256) W3: (256, 5) b3: (5,)\n",
      "out_layer: (?, 5)\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function, and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "# On this case we choose the AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.374258177\n",
      "Epoch: 0002 cost= 1.291216628\n",
      "Epoch: 0003 cost= 1.282853559\n",
      "Epoch: 0004 cost= 1.295923735\n",
      "Epoch: 0005 cost= 1.283388199\n",
      "Epoch: 0006 cost= 1.281021480\n",
      "Epoch: 0007 cost= 1.283265894\n",
      "Epoch: 0008 cost= 1.285428069\n",
      "Epoch: 0009 cost= 1.280745127\n",
      "Epoch: 0010 cost= 1.285867087\n",
      "Epoch: 0011 cost= 1.282388464\n",
      "Epoch: 0012 cost= 1.277082780\n",
      "Epoch: 0013 cost= 1.276581389\n",
      "Epoch: 0014 cost= 1.277192652\n",
      "Epoch: 0015 cost= 1.281592627\n",
      "Epoch: 0016 cost= 1.277672629\n",
      "Epoch: 0017 cost= 1.281152615\n",
      "Epoch: 0018 cost= 1.274752625\n",
      "Epoch: 0019 cost= 1.279632619\n",
      "Epoch: 0020 cost= 1.271712624\n",
      "Epoch: 0021 cost= 1.278912617\n",
      "Epoch: 0022 cost= 1.276392622\n",
      "Epoch: 0023 cost= 1.277952633\n",
      "Epoch: 0024 cost= 1.278592623\n",
      "Epoch: 0025 cost= 1.274512617\n",
      "Epoch: 0026 cost= 1.272792639\n",
      "Epoch: 0027 cost= 1.274192628\n",
      "Epoch: 0028 cost= 1.277272618\n",
      "Epoch: 0029 cost= 1.277432615\n",
      "Epoch: 0030 cost= 1.282072614\n",
      "Optimization Finished!\n",
      "Accuracy: 0.56533056\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(y_train_NN.size/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next_batch(batch_size,train_vecs_dbow_dmc,y_train_NN)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # To keep sizes compatible with model\n",
    "    print (\"Accuracy:\", accuracy.eval({x: validation_vecs_dbow_dmc, y: y_validation_NN}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build NN for _CountVectorizer_ and _TF-IDF Vectorizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (?, 8129) W1: (8129, 256) b1: (256,)\n",
      "layer_1: (?, 256) W2: (256, 256) b2: (256,)\n",
      "layer_2: (?, 256) W3: (256, 5) b3: (5,)\n",
      "out_layer: (?, 5)\n"
     ]
    }
   ],
   "source": [
    " # Parameters\n",
    "learning_rate = 0.002\n",
    "training_epochs = 10\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of features\n",
    "n_hidden_2 = 256 # 2nd layer number of features\n",
    "n_input = 8129 # 8129 inputs\n",
    "n_classes = 5 # 5 classes \n",
    "x = tf.placeholder(\"float\", [None, n_input]) \n",
    "y = tf.placeholder(\"float\", [None, n_classes])\n",
    "\n",
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "\n",
    "    print( 'x:', x.get_shape(), 'W1:', weights['h1'].get_shape(), 'b1:', biases['b1'].get_shape())        \n",
    "    # Hidden layer with linear activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1']) #(x*weights['h1']) + biases['b1']\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "\n",
    "    # Hidden layer with linear activation\n",
    "    print( 'layer_1:', layer_1.get_shape(), 'W2:', weights['h2'].get_shape(), 'b2:', biases['b2'].get_shape())        \n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']) # (layer_1 * weights['h2']) + biases['b2'] \n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "\n",
    "    # Output layer with softmax activation\n",
    "    print( 'layer_2:', layer_2.get_shape(), 'W3:', weights['out'].get_shape(), 'b3:', biases['out'].get_shape())        \n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out'] # (layer_2 * weights['out']) + biases['out']    \n",
    "    out_layer = tf.nn.softmax(out_layer)\n",
    "    print('out_layer:',out_layer.get_shape())\n",
    "\n",
    "    return out_layer\n",
    "\n",
    "weights = {\n",
    "'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),    #8129x256\n",
    "'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), #256x256\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_classes]))  #256x5\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),             #256x1\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),             #256x1\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))              #5x1\n",
    "}\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "# Cross entropy loss function\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=pred, labels=y))\n",
    "\n",
    "# On this case we choose the AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch CountVectorizer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.318430568\n",
      "Epoch: 0002 cost= 1.277712661\n",
      "Epoch: 0003 cost= 1.273192630\n",
      "Epoch: 0004 cost= 1.271032627\n",
      "Epoch: 0005 cost= 1.280632627\n",
      "Epoch: 0006 cost= 1.280912619\n",
      "Epoch: 0007 cost= 1.278752619\n",
      "Epoch: 0008 cost= 1.277312613\n",
      "Epoch: 0009 cost= 1.273312613\n",
      "Epoch: 0010 cost= 1.273752627\n",
      "Optimization Finished!\n",
      "Accuracy: 0.5663717\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(y_train_NN.size/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next_batch(batch_size,x_train_cntVectorized,y_train_NN)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # To keep sizes compatible with model\n",
    "    print (\"Accuracy:\", accuracy.eval({x: x_validation_cntVectorized, y: y_validation_NN}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Launch TF-IDF Vectorizer NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.292709162\n",
      "Epoch: 0002 cost= 1.275792626\n",
      "Epoch: 0003 cost= 1.277512617\n",
      "Epoch: 0004 cost= 1.272632621\n",
      "Epoch: 0005 cost= 1.271072628\n",
      "Epoch: 0006 cost= 1.275432628\n",
      "Epoch: 0007 cost= 1.272912616\n",
      "Epoch: 0008 cost= 1.280192622\n",
      "Epoch: 0009 cost= 1.273832618\n",
      "Epoch: 0010 cost= 1.279632617\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[i] for i in idx]\n",
    "    labels_shuffle = [labels[i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\n",
    "\n",
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        total_batch = int(y_train_NN.size/batch_size)\n",
    "        # Loop over all batches\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = next_batch(batch_size,x_train_tfidVectorized,y_train_NN)\n",
    "            # Run optimization op (backprop) and cost op (to get loss value)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict={x: batch_x,y: batch_y})\n",
    "            # Compute average loss\n",
    "            avg_cost += c / total_batch\n",
    "        # Display logs per epoch step\n",
    "        if epoch % display_step == 0:\n",
    "            print (\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \\\n",
    "                \"{:.9f}\".format(avg_cost))\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Test model\n",
    "    correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "    # Calculate accuracy\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    # To keep sizes compatible with model\n",
    "    print (\"Accuracy:\", accuracy.eval({x: x_validation_tfidVectorized, y: y_validation_NN}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
